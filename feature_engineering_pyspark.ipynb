{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Engineering with pySpark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Version\n",
    "Checking the version of which Spark and Python installed is important as it changes very quickly and drastically. Reading the wrong documentation can cause lots of lost time and unnecessary frustration!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Return spark version\n",
    "print(spark.version)\n",
    "\n",
    "# Return python version\n",
    "import sys\n",
    "print(sys.version_info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load in the data\n",
    "Reading in data is the first step to using PySpark for data science! Let's leverage the new industry standard of parquet files!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read the file into a dataframe\n",
    "df = spark.read.parquet('Real_Estate.parq')\n",
    "# Print columns in dataframe\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What are we predicting?\n",
    "Which of these fields (or columns) is the value we are trying to predict for?\n",
    "\n",
    "- TAXES\n",
    "- SALESCLOSEPRICE\n",
    "- DAYSONMARKET\n",
    "- LISTPRICE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select our dependent variable\n",
    "Y_df = df.select([\"SALESCLOSEPRICE\"])\n",
    "\n",
    "# Display summary statistics\n",
    "Y_df.describe().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying Data Load\n",
    "Let's suppose each month you get a new file. You know to expect a certain number of records and columns. In this exercise we will create a function that will validate the file loaded."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_load(df, num_records, num_columns):\n",
    "  # Takes a dataframe and compares record and column counts to input\n",
    "  # Message to return if the critera below aren't met\n",
    "  message = 'Validation Failed'\n",
    "  # Check number of records\n",
    "  if num_records == df.count():\n",
    "    # Check number of columns\n",
    "    if num_columns == len(df.columns):\n",
    "      # Success message\n",
    "      message = \"Validation Passed\"\n",
    "  return message\n",
    "\n",
    "# Print the data validation message\n",
    "print(check_load(df, 5000, 74))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Verifying DataTypes\n",
    "In the age of data we have access to more attributes than we ever had before. To handle all of them we will build a lot of automation but at a minimum requires that their datatypes be correct. In this exercise we will validate a dictionary of attributes and their datatypes to see if they are correct. This dictionary is stored in the variable validation_dict and is available in your workspace."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create list of actual dtypes to check\n",
    "actual_dtypes_list = df.dtypes\n",
    "print(actual_dtypes_list)\n",
    "\n",
    "# Iterate through the list of actual dtypes tuples\n",
    "for attribute_tuple in actual_dtypes_list:\n",
    "  \n",
    "  # Check if column name is dictionary of expected dtypes\n",
    "  col_name = attribute_tuple[0]\n",
    "  if col_name in validation_dict:\n",
    "\n",
    "    # Compare attribute types\n",
    "    col_type = attribute_tuple[1]\n",
    "    if col_type == validation_dict[col_name]:\n",
    "      print(col_name + ' has expected dtype.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Corr()\n",
    "The old adage 'Correlation does not imply Causation' is a cautionary tale. However, correlation does give us a good nudge to know where to start looking promising features to use in our models. Use this exercise to get a feel for searching through your data for the first time, trying to find patterns.\n",
    "\n",
    "A list called columns containing column names has been created for you. In this exercise you will compute the correlation between those columns and 'SALESCLOSEPRICE', and find the maximum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Name and value of col with max corr\n",
    "corr_max = 0\n",
    "corr_max_col = columns[0]\n",
    "\n",
    "# Loop to check all columns contained in list\n",
    "for col in columns:\n",
    "    # Check the correlation of a pair of columns\n",
    "    corr_val = df.corr(col, 'SALESCLOSEPRICE')\n",
    "    # Logic to compare corr_max with current corr_val\n",
    "    if corr_val > corr_max:\n",
    "        # Update the column name and corr value\n",
    "        corr_max = corr_val\n",
    "        corr_max_col = col\n",
    "\n",
    "print(corr_max_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Visualizations: distplot\n",
    "Understanding the distribution of our dependent variable is very important and can impact the type of model or preprocessing we do. A great way to do this is to plot it, however plotting is not a built in function in PySpark, we will need to take some intermediary steps to make sure it works correctly. In this exercise you will visualize the variable the 'LISTPRICE' variable, and you will gain more insights on its distribution by computing the skewness.\n",
    "\n",
    "The matplotlib.pyplot and seaborn packages have been imported for you with aliases plt and sns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a single column and sample and convert to pandas\n",
    "sample_df = df.select(['LISTPRICE']).sample(False, 0.5, 42)\n",
    "pandas_df = sample_df.toPandas()\n",
    "\n",
    "# Plot distribution of pandas_df and display plot\n",
    "sns.distplot(pandas_df)\n",
    "plt.show()\n",
    "\n",
    "# Import skewness function\n",
    "from pyspark.sql.functions import skewness\n",
    "\n",
    "# Compute and print skewness of LISTPRICE\n",
    "print(df.agg({\"LISTPRICE\": \"skewness\"}).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Visualizations: lmplot\n",
    "Creating linear model plots helps us visualize if variables have relationships with the dependent variable. If they do they are good candidates to include in our analysis. If they don't it doesn't mean that we should throw them out, it means we may have to process or wrangle them before they can be used.\n",
    "\n",
    "seaborn is available in your workspace with the customary alias sns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a the relevant columns and sample\n",
    "sample_df = df.select(['SALESCLOSEPRICE', 'LIVINGAREA']).sample(False, 0.5, 42)\n",
    "\n",
    "# Convert to pandas dataframe\n",
    "pandas_df = sample_df.toPandas()\n",
    "\n",
    "# Linear model plot of pandas_df\n",
    "sns.lmplot(x='LIVINGAREA', y='SALESCLOSEPRICE', data=pandas_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping a list of columns\n",
    "Our data set is rich with a lot of features, but not all are valuable. We have many that are going to be hard to wrangle into anything useful. For now, let's remove any columns that aren't immediately useful by dropping them.\n",
    "\n",
    "- 'STREETNUMBERNUMERIC': The postal address number on the home\n",
    "- 'FIREPLACES': Number of Fireplaces in the home\n",
    "- 'LOTSIZEDIMENSIONS': Free text describing the lot shape\n",
    "- 'LISTTYPE': Set list of values of sale type\n",
    "- 'ACRES': Numeric area of lot size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show top 30 records\n",
    "df.show(30)\n",
    "\n",
    "# List of columns to remove from dataset\n",
    "cols_to_drop = ['STREETNUMBERNUMERIC', 'LOTSIZEDIMENSIONS']\n",
    "\n",
    "# Drop columns in list\n",
    "df = df.drop(*cols_to_drop)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using text filters to remove records\n",
    "It pays to have to ask your clients lots of questions and take time to understand your variables. You find out that Assumable mortgage is an unusual occurrence in the real estate industry and your client suggests you exclude them. In this exercise we will use isin() which is similar to like() but allows us to pass a list of values to use as a filter rather than a single one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect unique values in the column 'ASSUMABLEMORTGAGE'\n",
    "df.select(['ASSUMABLEMORTGAGE']).distinct().show()\n",
    "\n",
    "# List of possible values containing 'yes'\n",
    "yes_values = ['Yes w/ Qualifying', 'Yes w/No Qualifying']\n",
    "\n",
    "# Filter the text values out of df but keep null values\n",
    "text_filter = ~df['ASSUMABLEMORTGAGE'].isin(yes_values) | df['ASSUMABLEMORTGAGE'].isNull()\n",
    "df = df.where(text_filter)\n",
    "\n",
    "# Print count of remaining records\n",
    "print(df.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering numeric fields conditionally\n",
    "Again, understanding the context of your data is extremely important. We want to understand what a normal range of houses sell for. Let's make sure we exclude any outlier homes that have sold for significantly more or less than the average. Here we will calculate the mean and standard deviation and use them to filer the near normal field log_SalesClosePrice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import mean, stddev\n",
    "\n",
    "# Calculate values used for outlier filtering\n",
    "mean_val = df.agg({'log_SalesClosePrice': \"mean\"}).collect()[0][0]\n",
    "stddev_val = df.agg({'log_SalesClosePrice': \"stddev\"}).collect()[0][0]\n",
    "\n",
    "# Create three standard deviation (μ ± 3σ) lower and upper bounds for data\n",
    "low_bound = mean_val - (3 * stddev_val)\n",
    "hi_bound = mean_val + (3 * stddev_val)\n",
    "\n",
    "# Filter the data to fit between the lower and upper bounds\n",
    "df = df.where((df['log_SalesClosePrice'] < hi_bound) & (df['log_SalesClosePrice'] > low_bound))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Custom Percentage Scaling\n",
    "In the slides we showed how to scale the data between 0 and 1. Sometimes you may wish to scale things differently for modeling or display purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define max and min values and collect them\n",
    "max_days = df.agg({\"DAYSONMARKET\": \"max\"}).collect()[0][0]\n",
    "min_days = df.agg({\"DAYSONMARKET\": \"min\"}).collect()[0][0]\n",
    "\n",
    "# Create a new column based off the scaled data\n",
    "df = df.withColumn('percentagescaleddays', \n",
    "                  round((df[\"DAYSONMARKET\"] - min_days) / (max_days - min_days)) * 100)\n",
    "\n",
    "# Calc max and min for new column\n",
    "print(df.agg({'percentagescaleddays': \"max\"}).collect())\n",
    "print(df.agg({'percentagescaleddays': \"min\"}).collect())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Scaling your scalers\n",
    "In the previous exercise, we minmax scaled a single variable. Suppose you have a LOT of variables to scale, you don't want hundreds of lines to code for each. Let's expand on the previous exercise and make it a function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def min_max_scaler(df, cols_to_scale):\n",
    "  # Takes a dataframe and list of columns to minmax scale. Returns a dataframe.\n",
    "  for col in cols_to_scale:\n",
    "    # Define min and max values and collect them\n",
    "    max_days = df.agg({col: 'max'}).collect()[0][0]\n",
    "    min_days = df.agg({col: 'min'}).collect()[0][0]\n",
    "    new_column_name = 'scaled_' + col\n",
    "    # Create a new column based off the scaled data\n",
    "    df = df.withColumn(new_column_name, \n",
    "                      (df[col] - min_days) / (max_days - min_days))\n",
    "  return df\n",
    "  \n",
    "df = min_max_scaler(df, cols_to_scale)\n",
    "# Show that our data is now between 0 and 1\n",
    "df[['DAYSONMARKET', 'scaled_DAYSONMARKET']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Correcting Right Skew Data\n",
    "In the slides we showed how you might use log transforms to fix positively skewed data (data whose distribution is mostly to the left). To correct negative skew (data mostly to the right) you need to take an extra step called \"reflecting\" before you can apply the inverse of , written as (1/) to make the data look more like normal a normal distribution. Reflecting data uses the following formula to reflect each value: ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import log\n",
    "\n",
    "# Compute the skewness\n",
    "print(df.agg({'YEARBUILT': \"skewness\"}).collect())\n",
    "\n",
    "# Calculate the max year\n",
    "max_year = df.agg({'YEARBUILT': \"max\"}).collect()[0][0]\n",
    "\n",
    "# Create a new column of reflected data\n",
    "df = df.withColumn('Reflect_YearBuilt', (max_year + 1) - df['YEARBUILT'])\n",
    "\n",
    "# Create a new column based reflected data\n",
    "df = df.withColumn('adj_yearbuilt', 1 / log(df['Reflect_YearBuilt']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing Missing Data\n",
    "Being able to plot missing values is a great way to quickly understand how much of your data is missing. It can also help highlight when variables are missing in a pattern something that will need to be handled with care lest your model be biased.\n",
    "\n",
    "Which variable has the most missing values? Run all lines of code except the last one to determine the answer. Once you're confident, and fill out the value and hit \"Submit Answer\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sample the dataframe and convert to Pandas\n",
    "sample_df = df.select(columns).sample(False, 0.1, 42)\n",
    "pandas_df = sample_df.toPandas()\n",
    "\n",
    "# Convert all values to T/F\n",
    "tf_df = pandas_df.isnull()\n",
    "\n",
    "# Plot it\n",
    "sns.heatmap(data=tf_df)\n",
    "plt.xticks(rotation=30, fontsize=10)\n",
    "plt.yticks(rotation=0, fontsize=10)\n",
    "plt.show()\n",
    "\n",
    "# Set the answer to the column with the most missing data\n",
    "answer = 'BACKONMARKETDATE'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imputing Missing Data\n",
    "Missing data happens. If we make the assumption that our data is missing completely at random, we are making the assumption that what data we do have, is a good representation of the population. If we have a few values we could remove them or we could use the mean or median as a replacement. In this exercise, we will look at 'PDOM': Days on Market at Current Price."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count missing rows\n",
    "missing = df.where(df['PDOM'].isNull()).count()\n",
    "\n",
    "# Calculate the mean value\n",
    "col_mean = df.agg({'PDOM': \"mean\"}).collect()[0][0]\n",
    "\n",
    "# Replacing with the mean value for that column\n",
    "df.fillna(col_mean, subset=['PDOM'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate Missing Percents\n",
    "Automation is the future of data science. Learning to automate some of your data preparation pays dividends. In this exercise, we will automate dropping columns if they are missing data beyond a specific threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def column_dropper(df, threshold):\n",
    "  # Takes a dataframe and threshold for missing values. Returns a dataframe.\n",
    "  total_records = df.count()\n",
    "  for col in df.columns:\n",
    "    # Calculate the percentage of missing values\n",
    "    missing = df.where(df[col].isNull()).count()\n",
    "    missing_percent = missing / total_records\n",
    "    # Drop column if percent of missing is more than threshold\n",
    "    if missing_percent > threshold:\n",
    "      df = df.drop(col)\n",
    "  return df\n",
    "\n",
    "# Drop columns that are more than 60% missing\n",
    "df = column_dropper(df, 0.6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A Dangerous Join\n",
    "In this exercise, we will be joining on Latitude and Longitude to bring in another dataset that measures how walk-friendly a neighborhood is. We'll need to be careful to make sure our joining columns are the same data type and ensure we are joining on the same precision (number of digits after the decimal) or our join won't work!\n",
    "\n",
    "Below you will find that df['latitude'] and df['longitude'] are at a higher precision than walk_df['longitude'] and walk_df['latitude'] we'll need to round them to the same precision so the join will work correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cast data types\n",
    "walk_df = walk_df.withColumn('longitude', walk_df[\"longitude\"].cast('double'))\n",
    "walk_df = walk_df.withColumn(\"latitude\", walk_df[\"latitude\"].cast('double'))\n",
    "\n",
    "# Round precision\n",
    "df = df.withColumn('longitude', round(df[\"longitude\"], 5))\n",
    "df = df.withColumn(\"latitude\", round(df[\"latitude\"], 5))\n",
    "\n",
    "# Create join condition\n",
    "condition = [df[\"latitude\"] == walk_df[\"latitude\"], df[\"longitude\"] == walk_df[\"longitude\"]]\n",
    "\n",
    "# Join the dataframes together\n",
    "join_df = df.join(walk_df, on=condition, how=\"left\")\n",
    "# Count non-null records from new field\n",
    "print(join_df.where(~join_df['walkscore'].isNull()).count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark SQL Join\n",
    "Sometimes it is much easier to write complex joins in SQL. In this exercise, we will start with the join keys already in the same format and precision but will use SparkSQL to do the joining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register dataframes as tables\n",
    "df.createOrReplaceTempView(\"df\")\n",
    "walk_df.createOrReplaceTempView(\"walk_df\")\n",
    "\n",
    "# SQL to join dataframes\n",
    "join_sql = \t\"\"\"\n",
    "\t\t\tSELECT \n",
    "\t\t\t\t*\n",
    "\t\t\tFROM df\n",
    "\t\t\tLEFT JOIN walk_df\n",
    "\t\t\tON df.longitude = walk_df.longitude\n",
    "\t\t\tAND df.latitude = walk_df.latitude\n",
    "\t\t\t\"\"\"\n",
    "# Perform sql join\n",
    "joined_df = spark.sql(join_sql)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for Bad Joins\n",
    "Joins can go bad silently if we are not careful, meaning they will not error out but instead return mangled data with more or less data than you'd intended. Let's take a look at a couple ways that joining incorrectly can change your data set for the worse.\n",
    "\n",
    "In this example we will look at what happens if you join two dataframes together when the join keys are not the same precision and compare the record counts between the correct join and the incorrect one."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join on mismatched keys precision \n",
    "wrong_prec_cond = [df_orig['longitude'] == walk_df['longitude'], df_orig['latitude'] == walk_df['latitude']]\n",
    "wrong_prec_df = df_orig.join(walk_df, on=wrong_prec_cond, how='left')\n",
    "\n",
    "# Compare bad join to the correct one\n",
    "print(wrong_prec_df.where(wrong_prec_df['walkscore'].isNull()).count())\n",
    "print(correct_join_df.where(correct_join_df['walkscore'].isNull()).count())\n",
    "\n",
    "# Create a join on too few keys\n",
    "few_keys_cond = [df['longitude'] == walk_df['longitude']]\n",
    "few_keys_df = df.join(walk_df, on=few_keys_cond, how='left')\n",
    "\n",
    "# Compare bad join to the correct one\n",
    "print(\"Record Count of the Too Few Keys Join Example: \" + str(few_keys_df.count()))\n",
    "print(\"Record Count of the Correct Join Example: \" + str(correct_join_df.count()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Differences\n",
    "Let's explore generating features using existing ones. In the midwest of the U.S. many single family homes have extra land around them for green space. In this example you will create a new feature called 'YARD_SIZE', and then see if the new feature is correlated with our outcome variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lot size in square feet\n",
    "acres_to_sqfeet = 43560\n",
    "df = df.withColumn(\"LOT_SIZE_SQFT\", df[\"ACRES\"] * acres_to_sqfeet)\n",
    "\n",
    "# Create new column YARD_SIZE\n",
    "df = df.withColumn(\"YARD_SIZE\", df[\"LOT_SIZE_SQFT\"] - df[\"FOUNDATIONSIZE\"])\n",
    "\n",
    "# Corr of ACRES vs SALESCLOSEPRICE\n",
    "print(\"Corr of ACRES vs SALESCLOSEPRICE: \" + str(df.corr(\"ACRES\", \"SALESCLOSEPRICE\")))\n",
    "# Corr of FOUNDATIONSIZE vs SALESCLOSEPRICE\n",
    "print(\"Corr of FOUNDATIONSIZE vs SALESCLOSEPRICE: \" + str(df.corr(\"FOUNDATIONSIZE\", \"SALESCLOSEPRICE\")))\n",
    "# Corr of YARD_SIZE vs SALESCLOSEPRICE\n",
    "print(\"Corr of YARD_SIZE vs SALESCLOSEPRICE: \" + str(df.corr(\"YARD_SIZE\", \"SALESCLOSEPRICE\")))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ratios\n",
    "Ratios are all around us. Whether it's miles per gallon or click through rate, they are everywhere. In this exercise, we'll create some ratios by dividing out pairs of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ASSESSED_TO_LIST\n",
    "df = df.withColumn(\"ASSESSED_TO_LIST\",df[\"ASSESSEDVALUATION\"]/df[\"LISTPRICE\"])\n",
    "df[['ASSESSEDVALUATION', 'LISTPRICE', 'ASSESSED_TO_LIST']].show(5)\n",
    "# TAX_TO_LIST\n",
    "df = df.withColumn(\"TAX_TO_LIST\",df[\"TAXES\"]/df[\"LISTPRICE\"])\n",
    "df[['TAX_TO_LIST', 'TAXES', 'LISTPRICE']].show(5)\n",
    "# BED_TO_BATHS\n",
    "df = df.withColumn(\"BED_TO_BATHS\",df[\"BEDROOMS\"]/df[\"BATHSTOTAL\"])\n",
    "df[['BED_TO_BATHS', 'BEDROOMS', 'BATHSTOTAL']].show(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deeper Features\n",
    "In previous exercises we showed how combining two features together can create good additional features for a predictive model. In this exercise, you will generate 'deeper' features by combining the effects of three variables into one. Then you will check to see if deeper and more complicated features always make for better predictors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new feature by adding two features together\n",
    "df = df.withColumn(\"Total_SQFT\", df[\"SQFTBELOWGROUND\"] + df[\"SQFTABOVEGROUND\"])\n",
    "\n",
    "# Create additional new feature using previously created feature\n",
    "df = df.withColumn(\"BATHS_PER_1000SQFT\", df[\"BATHSTOTAL\"] / (df[\"Total_SQFT\"] / 1000))\n",
    "df[[\"BATHS_PER_1000SQFT\"]].describe().show()\n",
    "\n",
    "# Sample and create pandas dataframe\n",
    "pandas_df = df.sample(False, 0.5, 0).toPandas()\n",
    "\n",
    "# Linear model plots\n",
    "sns.jointplot(x=\"Total_SQFT\", y=\"SALESCLOSEPRICE\", data=pandas_df, kind=\"reg\", stat_func=r2)\n",
    "plt.show()\n",
    "sns.jointplot(x=\"BATHS_PER_1000SQFT\", y=\"SALESCLOSEPRICE\", data=pandas_df, kind=\"reg\", stat_func=r2)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time Components\n",
    "Being able to work with time components for building features is important but you can also use them to explore and understand your data further. In this exercise, you'll be looking to see if there is a pattern to which day of the week a house lists on. Please keep in mind that PySpark's week starts on Sunday, with a value of 1 and ends on Saturday, a value of 7."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed functions\n",
    "from pyspark.sql.functions import to_date, dayofweek\n",
    "\n",
    "# Convert to date type\n",
    "df = df.withColumn(\"LISTDATE\", to_date(\"LISTDATE\"))\n",
    "\n",
    "# Get the day of the week\n",
    "df = df.withColumn(\"List_Day_of_Week\", dayofweek(\"LISTDATE\"))\n",
    "\n",
    "# Sample and convert to pandas dataframe\n",
    "sample_df = df.sample(False, 0.5, 42).toPandas()\n",
    "\n",
    "# Plot count plot of of day of week\n",
    "sns.countplot(x=\"List_Day_of_Week\", data=sample_df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining On Time Components\n",
    "Often times you will use date components to join in other sets of information. However, in this example, we need to use data that would have been available to those considering buying a house. This means we will need to use the previous year's reporting data for our analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import year\n",
    "\n",
    "# Initialize dataframes\n",
    "df = real_estate_df\n",
    "price_df = median_prices_df\n",
    "\n",
    "# Create year column\n",
    "df = df.withColumn(\"list_year\", year(\"LISTDATE\"))\n",
    "\n",
    "# Adjust year to match\n",
    "df = df.withColumn(\"report_year\", (df[\"list_year\"] - 1))\n",
    "\n",
    "# Create join condition\n",
    "condition = [df['CITY'] == price_df['City'], df['report_year'] == price_df['Year']]\n",
    "\n",
    "# Join the dataframes together\n",
    "df = df.join(price_df, on=condition, how=\"left\")\n",
    "# Inspect that new columns are available\n",
    "df[['MedianHomeValue']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Date Math\n",
    "In this example, we'll look at verifying the frequency of our data. The Mortgage dataset is supposed to have weekly data but let's make sure by lagging the report date and then taking the difference of the dates.\n",
    "\n",
    "Recall that to create a lagged feature we will need to create a window(). window() allows you to return a value for each record based off some calculation against a group of records, in this case, the previous period's mortgage rate."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lag, datediff, to_date\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Cast data type\n",
    "mort_df = mort_df.withColumn(\"DATE\", to_date(\"DATE\"))\n",
    "\n",
    "# Create window\n",
    "w = Window().orderBy(mort_df[\"DATE\"])\n",
    "# Create lag column\n",
    "mort_df = mort_df.withColumn(\"DATE-1\", lag(\"DATE\", count=1).over(w))\n",
    "\n",
    "# Calculate difference between date columns\n",
    "mort_df = mort_df.withColumn(\"Days_Between_Report\", datediff(\"DATE\", \"DATE-1\"))\n",
    "# Print results\n",
    "mort_df.select('Days_Between_Report').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting Text to New Features\n",
    "Garages are an important consideration for houses in Minnesota where most people own a car and the snow is annoying to clear off a car parked outside. The type of garage is also important, can you get to your car without braving the cold or not? Let's look at creating a feature has_attached_garage that captures whether the garage is attached to the house or not."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed functions\n",
    "from pyspark.sql.functions import when\n",
    "\n",
    "# Create boolean conditions for string matches\n",
    "has_attached_garage = df['GARAGEDESCRIPTION'].like(\"%Attached Garage%\")\n",
    "has_detached_garage = df['GARAGEDESCRIPTION'].like(\"%Detached Garage%\")\n",
    "\n",
    "# Conditional value assignment \n",
    "df = df.withColumn(\"has_attached_garage\", (when(has_attached_garage, 1)\n",
    "                                          .when(has_detached_garage, 0)\n",
    "                                          .otherwise(None)))\n",
    "\n",
    "# Inspect results\n",
    "df[['GARAGEDESCRIPTION', 'has_attached_garage']].show(truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting & Exploding\n",
    "Being able to take a compound field like GARAGEDESCRIPTION and massaging it into something useful is an involved process. It's helpful to understand early what value you might gain out of expanding it. In this example, we will convert our string to a list-like array, explode it and then inspect the unique values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import needed functions\n",
    "from pyspark.sql.functions import split,explode\n",
    "\n",
    "# Convert string to list-like array\n",
    "df = df.withColumn(\"garage_list\", split(df['GARAGEDESCRIPTION'], \", \"))\n",
    "\n",
    "# Explode the values into new records\n",
    "ex_df = df.withColumn(\"ex_garage_list\", explode(df['garage_list']))\n",
    "\n",
    "# Inspect the values\n",
    "ex_df[['ex_garage_list']].distinct().show(100, truncate=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivot & Join\n",
    "Being able to explode and pivot a compound field is great, but you are left with a dataframe of only those pivoted values. To really be valuable you'll need to rejoin it to the original dataset! After joining the datasets we will have a lot of NULL values for the newly created columns since we know the context of how they were created we can safely fill them in with zero as either the new has an attribute or it doesn't."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import coalesce, first\n",
    "\n",
    "# Pivot \n",
    "piv_df = ex_df.groupBy(\"NO\").pivot(\"ex_garage_list\").agg(coalesce(first(\"constant_val\")))\n",
    "\n",
    "# Join the dataframes together and fill null\n",
    "joined_df = df.join(piv_df, on='NO', how=\"left\")\n",
    "\n",
    "# Columns to zero fill\n",
    "zfill_cols = piv_df.columns\n",
    "\n",
    "# Zero fill the pivoted values\n",
    "zfilled_df = joined_df.fillna(0, subset=zfill_cols) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Binarizing Day of Week\n",
    "In a previous video, we saw that it was very unlikely for a home to list on the weekend. Let's create a new field that says if the house is listed for sale on a weekday or not. In this example there is a field called List_Day_of_Week that has Monday is labeled 1.0 and Sunday is 7.0. Let's convert this to a binary field with weekday being 0 and weekend being 1. We can use the pyspark feature transformer Binarizer to do this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import transformer\n",
    "from pyspark.ml.feature import Binarizer\n",
    "\n",
    "# Create the transformer\n",
    "binarizer = Binarizer(threshold=5.0 , inputCol=\"List_Day_of_Week\", outputCol=\"Listed_On_Weekend\")\n",
    "\n",
    "# Apply the transformation to df\n",
    "df = binarizer.transform(df)\n",
    "\n",
    "# Verify transformation\n",
    "df[[\"List_Day_of_Week\", \"Listed_On_Weekend\"]].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bucketing\n",
    "If you are a homeowner its very important if a house has 1, 2, 3 or 4 bedrooms. But like bathrooms, once you hit a certain point you don't really care whether the house has 7 or 8. This example we'll look at how to figure out where are some good value points to bucket."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import Bucketizer\n",
    "\n",
    "# Plot distribution of sample_df\n",
    "sns.distplot(sample_df, axlabel='BEDROOMS')\n",
    "plt.show()\n",
    "\n",
    "# Create the bucket splits and bucketizer\n",
    "splits = [0, 1, 2, 3, 4, 5, float('Inf')]\n",
    "buck = Bucketizer(splits=splits, inputCol=\"BEDROOMS\", outputCol=\"bedrooms\")\n",
    "\n",
    "# Apply the transformation to df: df_bucket\n",
    "df_bucket = buck.transform(df)\n",
    "\n",
    "# Display results\n",
    "df_bucket[['BEDROOMS', 'bedrooms']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One Hot Encoding\n",
    "In the United States where you live determines which schools your kids can attend. Therefore it's understandable that many people care deeply about which school districts their future home will be in. While the school districts are numbered in SCHOOLDISTRICTNUMBER they are really categorical. Meaning that summing or averaging these values has no apparent meaning. Therefore in this example we will convert SCHOOLDISTRICTNUMBER from a categorial variable into a numeric vector to use in our machine learning model later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import OneHotEncoder, StringIndexer\n",
    "\n",
    "# Map strings to numbers with string indexer\n",
    "string_indexer = StringIndexer(inputCol=\"SCHOOLDISTRICTNUMBER\", outputCol=\"School_Index\")\n",
    "indexed_df = string_indexer.fit(df).transform(df)\n",
    "\n",
    "# Onehot encode indexed values\n",
    "encoder = OneHotEncoder(inputCol=\"School_Index\", outputCol=\"School_Vec\")\n",
    "encoded_df = encoder.transform(indexed_df)\n",
    "\n",
    "# Inspect the transformation steps\n",
    "encoded_df[['SCHOOLDISTRICTNUMBER', 'School_Index', 'School_Vec']].show(truncate=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating Time Splits\n",
    "In the video, we learned why splitting data randomly can be dangerous for time series as data from the future can cause overfitting in our model. Often with time series, you acquire new data as it is made available and you will want to retrain your model using the newest data. In the video, we showed how to do a percentage split for test and training sets but suppose you wish to train on all available data except for the last 45days which you want to use for a test set.\n",
    "\n",
    "In this exercise, we will create a function to find the split date for using the last 45 days of data for testing and the rest for training. Please note that timedelta() has already been imported for you from the standard python library datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split_date(df, split_col, test_days=45):\n",
    "  \"\"\"Calculate the date to split test and training sets\"\"\"\n",
    "  # Find how many days our data spans\n",
    "  max_date = df.agg({split_col: \"max\"}).collect()[0][0]\n",
    "  min_date = df.agg({split_col: \"min\"}).collect()[0][0]\n",
    "  # Subtract an integer number of days from the last date in dataset\n",
    "  split_date = max_date - timedelta(days=test_days)\n",
    "  return split_date\n",
    "\n",
    "# Find the date to use in spitting test and train\n",
    "split_date = train_test_split_date(df, \"OFFMKTDATE\")\n",
    "\n",
    "# Create Sequential Test and Training Sets\n",
    "train_df = df.where(df[\"OFFMKTDATE\"] < split_date) \n",
    "test_df = df.where(df[\"OFFMKTDATE\"] >= split_date).where(df[\"LISTDATE\"] <= split_date) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adjusting Time Features\n",
    "We have mentioned throughout this course some of the dangers of leaking information to your model during training. Data leakage will cause your model to have very optimistic metrics for accuracy but once real data is run through it the results are often very disappointing.\n",
    "\n",
    "In this exercise, we are going to ensure that DAYSONMARKET only reflects what information we have at the time of predicting the value. I.e., if the house is still on the market, we don't know how many more days it will stay on the market. We need to adjust our test_df to reflect what information we currently have as of 2017-12-10.\n",
    "\n",
    "NOTE: This example will use the lit() function. This function is used to allow single values where an entire column is expected in a function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import datediff, to_date, lit\n",
    "\n",
    "split_date = to_date(lit('2017-12-10'))\n",
    "# Create Sequential Test set\n",
    "test_df = df.where(df[\"OFFMKTDATE\"] >= split_date).where(df[\"LISTDATE\"] <= split_date)\n",
    "\n",
    "# Create a copy of DAYSONMARKET to review later\n",
    "test_df = test_df.withColumn('DAYSONMARKET_Original', test_df['DAYSONMARKET'])\n",
    "\n",
    "# Recalculate DAYSONMARKET from what we know on our split date\n",
    "test_df = test_df.withColumn(\"DAYSONMARKET\", datediff(split_date, \"LISTDATE\"))\n",
    "\n",
    "# Review the difference\n",
    "test_df[['LISTDATE', 'OFFMKTDATE', 'DAYSONMARKET_Original', 'DAYSONMARKET']].show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping Columns with Low Observations\n",
    "After doing a lot of feature engineering it's a good idea to take a step back and look at what you've created. If you've used some automation techniques on your categorical features like exploding or OneHot Encoding you may find that you now have hundreds of new binary features. While the subject of feature selection is material for a whole other course but there are some quick steps you can take to reduce the dimensionality of your data set.\n",
    "\n",
    "In this exercise, we are going to remove columns that have less than 30 observations. 30 is a common minimum number of observations for statistical significance. Any less than that and the relationships cause overfitting because of a sheer coincidence!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs_threshold = 30\n",
    "cols_to_remove = list()\n",
    "# Inspect first 10 binary columns in list\n",
    "for col in binary_cols[0:10]:\n",
    "  # Count the number of 1 values in the binary column\n",
    "  obs_count = df.agg({col: \"sum\"}).collect()[0][0]\n",
    "  # If less than our observation threshold, remove\n",
    "  if obs_count < obs_threshold:\n",
    "    cols_to_remove.append(col)\n",
    "    \n",
    "# Drop columns and print starting and ending dataframe shapes\n",
    "new_df = df.drop(*cols_to_remove)\n",
    "\n",
    "print('Rows: ' + str(df.count()) + ' Columns: ' + str(len(df.columns)))\n",
    "print('Rows: ' + str(new_df.count()) + ' Columns: ' + str(len(new_df.columns)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Naively Handling Missing and Categorical Values\n",
    "Random Forest Regression is robust enough to allow us to ignore many of the more time consuming and tedious data preparation steps. While some implementations of Random Forest handle missing and categorical values automatically, PySpark's does not. The math remains the same however so we can get away with some naive value replacements.\n",
    "\n",
    "For missing values since our data is strictly positive, we will assign -1. The random forest will split on this value and handle it differently than the rest of the values in the same feature.\n",
    "\n",
    "For categorical values, we can just map the text values to numbers and again the random forest will appropriately handle them by splitting on them. In this example, we will dust off pipelines from Introduction to PySpark to write our code more concisely. Please note that the exercise will start by displaying the dtypes of the columns in the dataframe, compare them to the results at the end of this exercise."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace missing values\n",
    "df = df.fillna(-1, subset=[\"WALKSCORE\", \"BIKESCORE\"])\n",
    "\n",
    "# Create list of StringIndexers using list comprehension\n",
    "indexers = [StringIndexer(inputCol=col, outputCol=col+\"_IDX\")\\\n",
    "            .setHandleInvalid(\"keep\") for col in categorical_cols]\n",
    "# Create pipeline of indexers\n",
    "indexer_pipeline = Pipeline(stages=indexers)\n",
    "# Fit and Transform the pipeline to the original data\n",
    "df_indexed =indexer_pipeline.fit(df).transform(df)\n",
    "\n",
    "# Clean up redundant columns\n",
    "df_indexed = df_indexed.drop(*categorical_cols)\n",
    "# Inspect data transformations\n",
    "print(df_indexed.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building a Regression Model\n",
    "One of the great things about PySpark ML module is that most algorithms can be tried and tested without changing much code. Random Forest Regression is a fairly simple ensemble model, using bagging to fit. Another tree based ensemble model is Gradient Boosted Trees which uses a different approach called boosting to fit. In this exercise let's train a GBTRegressor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor\n",
    "\n",
    "# Train a Gradient Boosted Trees (GBT) model.\n",
    "gbt = GBTRegressor(featuresCol=\"features\",\n",
    "                           labelCol=\"SALESCLOSEPRICE\",\n",
    "                           predictionCol=\"Prediction_Price\",\n",
    "                           seed=42\n",
    "                           )\n",
    "\n",
    "# Train model.\n",
    "model = gbt.fit(train_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating & Comparing Algorithms\n",
    "Now that we've created a new model with GBTRegressor its time to compare it against our baseline of RandomForestRegressor. To do this we will compare the predictions of both models to the actual data and calculate RMSE and R^2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "\n",
    "# Select columns to compute test error\n",
    "evaluator = RegressionEvaluator(labelCol=\"SALESCLOSEPRICE\", \n",
    "                                predictionCol=\"Prediction_Price\")\n",
    "# Dictionary of model predictions to loop over\n",
    "models = {'Gradient Boosted Trees': gbt_predictions, 'Random Forest Regression': rfr_predictions}\n",
    "for key, preds in models.items():\n",
    "  # Create evaluation metrics\n",
    "  rmse = evaluator.evaluate(preds, {evaluator.metricName: \"rmse\"})\n",
    "  r2 = evaluator.evaluate(preds, {evaluator.metricName: \"r2\"})\n",
    "  \n",
    "  # Print Model Metrics\n",
    "  print(key + ' RMSE: ' + str(rmse))\n",
    "  print(key + ' R^2: ' + str(r2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Interpreting Results\n",
    "It is almost always important to know which features are influencing your prediction the most. Perhaps its counterintuitive and that's an insight? Perhaps a hand full of features account for most of the accuracy of your model and you don't need to perform time acquiring or massaging other features.\n",
    "\n",
    "In this example we will be looking at a model that has been trained without any LISTPRICE information. With that gone, what influences the price the most?\n",
    "\n",
    "- NOTE: The array of feature importances, importances has already been created for you from model.featureImportances.toArray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert feature importances to a pandas column\n",
    "fi_df = pd.DataFrame(importances, columns=[\"importance\"])\n",
    "\n",
    "# Convert list of feature names to pandas column\n",
    "fi_df['feature'] = pd.Series(feature_cols)\n",
    "\n",
    "# Sort the data based on feature importance\n",
    "fi_df.sort_values(by=[\"importance\"], ascending=False, inplace=True)\n",
    "\n",
    "# Inspect Results\n",
    "fi_df.head(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving & Loading Models\n",
    "Often times you may find yourself going back to a previous model to see what assumptions or settings were used when diagnosing where your prediction errors were coming from. Perhaps there was something wrong with the data? Maybe you need to incorporate a new feature to capture an unusual event that occurred?\n",
    "\n",
    "In this example, you will practice saving and loading a model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressionModel\n",
    "\n",
    "# Save model\n",
    "model.save(\"rfr_no_listprice\")\n",
    "\n",
    "# Load model\n",
    "loaded_model = RandomForestRegressionModel.load(\"rfr_no_listprice\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
